# ğŸš€ Advanced Stock Predictor with Transformer + LSTM Ensemble

## ğŸ“‹ Overview
Sophisticated stock price prediction system combining PyTorch Transformer and Enhanced LSTM models with comprehensive technical analysis and interactive visualizations.

## ğŸ¯ Quick Start

### 1. Setup Environment
```bash
python -m venv stock_env
source stock_env/bin/activate  # On macOS/Linux
# stock_env\Scripts\activate  # On Windows
pip install --upgrade pip
```

### 2. Install All Dependencies
```bash
pip install -r requirements.txt
```

### 3. Run the System
```bash
python src/ensemble_model.py
```

## ğŸ“Š **COMPLETE TECHNICAL SPECIFICATION**

### ğŸ”¬ **Data Source & Collection**
- **Primary Source**: Yahoo Finance (YFinance API)
- **Data Type**: Daily OHLCV (Open, High, Low, Close, Volume)
- **Symbols Supported**: Any publicly traded stock (AAPL, GOOGL, MSFT, NVDA, etc.)
- **Historical Range**: Automatically fetches maximum available history
- **Data Quality**: Real-time, no simulated or synthetic data

### ğŸ“ˆ **Data Splitting Strategy**
```
ğŸ—“ï¸ CHRONOLOGICAL SPLIT (Time-Series Aware):
â”œâ”€â”€ Training Data:   64% (oldest data)
â”œâ”€â”€ Validation Data: 16% (middle period) 
â””â”€â”€ Test Data:       20% (most recent data)

Example with 1000 trading days:
â”œâ”€â”€ Training:   Days 1-640   (for learning patterns)
â”œâ”€â”€ Validation: Days 641-800 (for model selection during training)
â””â”€â”€ Test:       Days 801-1000 (for final evaluation)

ğŸ”„ SEQUENCE CONFIGURATION:
- Sequence Length: 60 trading days lookback
- Prediction Target: Next day's closing price
- Input Shape: (batch_size, 60, 21)
```

### ğŸ§  **AI Architecture Deep Dive**

#### **ğŸ¯ Ensemble Strategy**
- **Primary Model**: PyTorch Transformer (75% weight)
- **Secondary Model**: PyTorch Enhanced LSTM (25% weight)
- **Combination Method**: Adaptive weighted ensemble
- **Weight Adjustment**: Dynamic based on recent performance

#### **ğŸ”® Transformer Model (Primary Predictor)**
```python
Architecture:
â”œâ”€â”€ Input Projection: Linear(21 â†’ 128)
â”œâ”€â”€ Positional Encoding: Sinusoidal (max_len=100)
â”œâ”€â”€ Transformer Encoder: 4 layers
â”‚   â”œâ”€â”€ Multi-Head Attention: 8 heads
â”‚   â”œâ”€â”€ Feed-Forward: 128 â†’ 512 â†’ 128
â”‚   â”œâ”€â”€ Dropout: 0.1
â”‚   â””â”€â”€ Activation: GELU
â”œâ”€â”€ Output Projection: Linear(128 â†’ 64)
â””â”€â”€ Quantile Heads: 3 outputs (10%, 50%, 90% quantiles)

Training Configuration:
â”œâ”€â”€ Loss Function: Quantile Loss (for uncertainty)
â”œâ”€â”€ Optimizer: AdamW (lr=0.001, weight_decay=0.01)
â”œâ”€â”€ Scheduler: ReduceLROnPlateau
â”œâ”€â”€ Epochs: 50 (default)
â”œâ”€â”€ Batch Size: 32
â”œâ”€â”€ Early Stopping: 15 epochs patience
â””â”€â”€ Gradient Clipping: max_norm=1.0
```

#### **ğŸ”„ Enhanced LSTM Model (Secondary Predictor)**
```python
Architecture:
â”œâ”€â”€ Input Normalization: BatchNorm1d
â”œâ”€â”€ Bidirectional LSTM: 2 layers Ã— 128 hidden units
â”‚   â”œâ”€â”€ Dropout: 0.3 between layers
â”‚   â””â”€â”€ Forget Gate Bias: 1.0 (for better gradient flow)
â”œâ”€â”€ Batch Normalization: After LSTM
â”œâ”€â”€ Dense Layers: 256 â†’ 128 â†’ 64 â†’ 1
â”‚   â”œâ”€â”€ Activation: ELU (better than ReLU for negative values)
â”‚   â”œâ”€â”€ Dropout: 0.4
â”‚   â””â”€â”€ Residual Connections: Skip connections for deep layers
â””â”€â”€ Weight Initialization: Xavier/Glorot uniform

Training Configuration:
â”œâ”€â”€ Loss Function: HuberLoss (delta=0.1, robust to outliers)
â”œâ”€â”€ Optimizer: AdamW (lr=0.001, weight_decay=0.01)
â”œâ”€â”€ Scheduler: CosineAnnealingWarmRestarts (T_0=10, T_mult=2)
â”œâ”€â”€ Epochs: 100 (with early stopping)
â”œâ”€â”€ Batch Size: 32
â”œâ”€â”€ Early Stopping: 20 epochs patience
â”œâ”€â”€ Gradient Clipping: max_norm=1.0
â””â”€â”€ Best Model Restoration: Saves best validation performance
```

### ğŸ› ï¸ **Feature Engineering (21 Enhanced Features)**

#### **ğŸ“Š Technical Indicators (9 features)**
```python
1. RSI (14-period): Relative Strength Index
2. MACD Line: 12-EMA - 26-EMA  
3. MACD Signal: 9-EMA of MACD
4. MACD Histogram: MACD - Signal
5. Bollinger Position: (Price - Lower) / (Upper - Lower)
6. On-Balance Volume: Cumulative volume flow
7. Volume Ratio: Current / 20-day average volume
8. Price Change 1D: (Close - Previous Close) / Previous Close
9. Price Change 5D: 5-day momentum
```

#### **â° Time-Based Features (4 features)**
```python
10. Day of Week (Sin): sin(2Ï€ Ã— day / 7)
11. Day of Week (Cos): cos(2Ï€ Ã— day / 7)  
12. Month (Sin): sin(2Ï€ Ã— month / 12)
13. Month (Cos): cos(2Ï€ Ã— month / 12)
```

#### **ğŸ“ˆ Market Regime Indicators (2 features)**
```python
14. Volatility Regime: Rolling 20-day standard deviation
15. Trend Strength: Directional movement indicator
```

#### **ğŸ“‹ Base OHLCV Features (5 features)**
```python
16. Open Price (normalized)
17. High Price (normalized)
18. Low Price (normalized) 
19. Close Price (normalized)
20. Volume (normalized)
```

#### **ğŸ’ª Price Momentum (1 feature)**
```python
21. Multi-timeframe Momentum: Combined 1D, 5D, 20D momentum
```

### ğŸ”§ **Technology Stack & Why We Chose Each**

#### **ğŸ”¥ PyTorch Framework**
**Why PyTorch over TensorFlow?**
- âœ… **Dynamic Computation Graphs**: Better for research and experimentation
- âœ… **Pythonic Design**: More intuitive debugging and development
- âœ… **Superior Transformer Support**: Native attention mechanisms
- âœ… **Better Memory Management**: For large sequence models
- âœ… **Academic Standard**: Latest research uses PyTorch
- âœ… **Unified Framework**: Both models in same ecosystem

#### **ğŸ¯ Why Transformer as Primary Model?**
```python
Advantages for Stock Prediction:
â”œâ”€â”€ Self-Attention Mechanism: Captures long-range dependencies
â”œâ”€â”€ Parallel Processing: Faster training than RNNs
â”œâ”€â”€ No Vanishing Gradients: Better for long sequences
â”œâ”€â”€ Multi-Head Attention: Focuses on different aspects simultaneously
â”œâ”€â”€ Positional Encoding: Understands time relationships
â””â”€â”€ Uncertainty Quantification: Built-in confidence intervals
```

#### **ğŸ”„ Why LSTM as Secondary Model?**
```python
Advantages for Time Series:
â”œâ”€â”€ Sequential Processing: Natural for time series data
â”œâ”€â”€ Memory Cells: Remembers important past information
â”œâ”€â”€ Bidirectional: Sees both past and future context
â”œâ”€â”€ Robust to Noise: Good for volatile financial data
â”œâ”€â”€ Complementary: Different approach than Transformer
â””â”€â”€ Proven Track Record: Established success in finance
```

#### **âš–ï¸ Why Ensemble Both Models?**
```python
Ensemble Benefits:
â”œâ”€â”€ Reduced Overfitting: Multiple perspectives
â”œâ”€â”€ Better Generalization: Combines different strengths
â”œâ”€â”€ Uncertainty Reduction: More robust predictions
â”œâ”€â”€ Risk Mitigation: If one model fails, other compensates
â””â”€â”€ Performance Boost: Often 5-15% improvement over single models
```

### ğŸ“Š **Data Preprocessing Pipeline**

#### **ğŸ”„ Step-by-Step Process**
```python
1. Data Collection:
   â””â”€â”€ YFinance API â†’ Raw OHLCV data

2. Data Cleaning:
   â”œâ”€â”€ Sort by date (chronological order)
   â”œâ”€â”€ Remove missing values (forward/backward fill)
   â””â”€â”€ Validate data integrity

3. Feature Engineering:
   â”œâ”€â”€ Calculate technical indicators (TA-Lib)
   â”œâ”€â”€ Add time-based features (cyclical encoding)
   â”œâ”€â”€ Compute market regime indicators
   â””â”€â”€ Create momentum features

4. Normalization:
   â”œâ”€â”€ MinMaxScaler (0 to 1 range)
   â”œâ”€â”€ Fit on training data only
   â””â”€â”€ Transform all splits consistently

5. Sequence Creation:
   â”œâ”€â”€ Create 60-day lookback windows
   â”œâ”€â”€ Target: Next day closing price
   â””â”€â”€ Sliding window approach

6. Data Loading:
   â”œâ”€â”€ PyTorch DataLoader
   â”œâ”€â”€ Batch size: 32
   â””â”€â”€ Shuffle training, preserve order for validation/test
```

### ğŸ¨ **Visualization & Analysis**

#### **ğŸ“Š Interactive Plotly Visualizations**
```python
Generated Files:
â”œâ”€â”€ {SYMBOL}_dashboard.html: Complete 6-panel dashboard
â”‚   â”œâ”€â”€ Main prediction chart with confidence bands
â”‚   â”œâ”€â”€ Prediction error analysis over time
â”‚   â”œâ”€â”€ Model performance comparison (bar charts)
â”‚   â”œâ”€â”€ Uncertainty coverage analysis
â”‚   â”œâ”€â”€ Feature importance ranking (top 10)
â”‚   â””â”€â”€ Prediction accuracy distribution
â”œâ”€â”€ {SYMBOL}_simple.html: Clean prediction chart
â””â”€â”€ {SYMBOL}_results.md: Performance summary text

Features:
â”œâ”€â”€ Interactive: Hover tooltips, zoom, pan
â”œâ”€â”€ Standalone: No internet required
â”œâ”€â”€ Professional: Publication-ready styling
â”œâ”€â”€ Mobile-friendly: Responsive design
â””â”€â”€ Shareable: Send HTML files to anyone
```

#### **ğŸ“ˆ Performance Metrics**
```python
Evaluation Metrics:
â”œâ”€â”€ Mean Absolute Error (MAE): Average prediction error in $
â”œâ”€â”€ Root Mean Square Error (RMSE): Penalizes large errors
â”œâ”€â”€ Mean Absolute Percentage Error (MAPE): Relative error %
â”œâ”€â”€ Directional Accuracy: % of correct up/down predictions
â”œâ”€â”€ Confidence Coverage: % of actuals within prediction bands
â””â”€â”€ Ensemble Improvement: % better than individual models
```

### ğŸ—ï¸ **Project Structure**
```
stocks/
â”œâ”€â”€ requirements.txt          # All 60+ dependencies with versions
â”œâ”€â”€ README.md                # This comprehensive guide
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ensemble_model.py    # ğŸ¯ Main system (431 lines)
â”‚   â”œâ”€â”€ transformer_model.py # PyTorch Transformer (177 lines)
â”‚   â”œâ”€â”€ pytorch_lstm.py      # Enhanced LSTM (252 lines)
â”‚   â”œâ”€â”€ multi_modal.py       # Feature engineering (162 lines)
â”‚   â”œâ”€â”€ interactive_plots.py # Plotly visualizations (365 lines)
â”‚   â”œâ”€â”€ feature_analysis.py  # Feature importance (188 lines)
â”‚   â””â”€â”€ data_preprocessing.py # Data pipeline (75 lines)
â”œâ”€â”€ data/                    # Stock data cache (auto-created)
â”œâ”€â”€ models/                  # Trained model weights (auto-created)
â””â”€â”€ visualizations/          # Interactive charts (auto-created)
```

### âš™ï¸ **Training Process Details**

#### **ğŸ”„ Complete Training Pipeline**
```python
1. Data Preparation:
   â”œâ”€â”€ Fetch stock data (YFinance)
   â”œâ”€â”€ Engineer 21 features
   â”œâ”€â”€ Split: 64% train, 16% val, 20% test
   â””â”€â”€ Create sequences (60-day windows)

2. Model Initialization:
   â”œâ”€â”€ Transformer: 4 layers, 8 heads, 128 d_model
   â”œâ”€â”€ LSTM: 2 bidirectional layers, 128 hidden
   â””â”€â”€ Load pre-trained weights if available

3. Training Loop:
   â”œâ”€â”€ Transformer Training (50 epochs):
   â”‚   â”œâ”€â”€ Quantile loss for uncertainty
   â”‚   â”œâ”€â”€ AdamW optimizer
   â”‚   â”œâ”€â”€ Early stopping (15 epochs patience)
   â”‚   â””â”€â”€ Save best validation model
   â””â”€â”€ LSTM Training (100 epochs):
       â”œâ”€â”€ Huber loss (robust to outliers)
       â”œâ”€â”€ Cosine annealing scheduler
       â”œâ”€â”€ Early stopping (20 epochs patience)
       â””â”€â”€ Gradient clipping for stability

4. Ensemble Creation:
   â”œâ”€â”€ Load best individual models
   â”œâ”€â”€ Set weights: 75% Transformer, 25% LSTM
   â”œâ”€â”€ Adaptive weight adjustment based on performance
   â””â”€â”€ Save complete ensemble

5. Final Evaluation:
   â”œâ”€â”€ Predict on test set (never seen during training)
   â”œâ”€â”€ Calculate performance metrics
   â”œâ”€â”€ Generate interactive visualizations
   â””â”€â”€ Create results summary
```

### ğŸ” **Model Validation Strategy**

#### **ğŸ¯ Why Our Approach Works**
```python
Time Series Validation:
â”œâ”€â”€ No Random Shuffling: Preserves temporal order
â”œâ”€â”€ Chronological Split: Realistic trading scenario
â”œâ”€â”€ Walk-Forward Validation: Simulates real-world deployment
â”œâ”€â”€ Out-of-Sample Testing: True unseen data evaluation
â””â”€â”€ Multiple Metrics: Comprehensive performance assessment

Overfitting Prevention:
â”œâ”€â”€ Early Stopping: Prevents memorization
â”œâ”€â”€ Dropout Regularization: Random neuron deactivation
â”œâ”€â”€ Weight Decay: L2 regularization penalty
â”œâ”€â”€ Batch Normalization: Stable training dynamics
â””â”€â”€ Validation Monitoring: Real-time performance tracking
```

### ğŸš€ **Performance Optimizations**

#### **âš¡ Speed & Memory Optimizations**
```python
Training Optimizations:
â”œâ”€â”€ Gradient Clipping: Prevents exploding gradients
â”œâ”€â”€ Mixed Precision: Faster training (if GPU available)
â”œâ”€â”€ Batch Processing: Efficient parallel computation
â”œâ”€â”€ Model Checkpointing: Resume interrupted training
â””â”€â”€ Early Stopping: Avoid unnecessary computation

Memory Management:
â”œâ”€â”€ Sequence Chunking: Process data in manageable pieces
â”œâ”€â”€ Gradient Accumulation: Simulate larger batch sizes
â”œâ”€â”€ Model Pruning: Remove unnecessary parameters
â””â”€â”€ Efficient Data Loading: Minimize RAM usage
```

### ğŸ“‹ **System Requirements**
```python
Minimum Requirements:
â”œâ”€â”€ Python: 3.9+
â”œâ”€â”€ RAM: 4GB (8GB+ recommended)
â”œâ”€â”€ Storage: 2GB free space
â”œâ”€â”€ Internet: For initial data download
â””â”€â”€ OS: Windows 10+, macOS 10.14+, Ubuntu 18.04+

Recommended Setup:
â”œâ”€â”€ RAM: 16GB+ 
â”œâ”€â”€ CPU: 8+ cores
â”œâ”€â”€ GPU: CUDA-compatible (optional, auto-detected)
â””â”€â”€ SSD: For faster data loading
```

### ğŸ”§ **Installation & Troubleshooting**

#### **ğŸ“¦ Dependencies (60+ packages)**
```bash
# Core ML Stack
torch>=2.8.0, pandas, numpy, scikit-learn

# Financial Data
yfinance, pandas-datareader, alpha-vantage

# Technical Analysis  
TA-Lib (may need system installation)

# Visualization
plotly, matplotlib, seaborn

# Advanced ML
xgboost, lightgbm, optuna

# Web & API
streamlit, fastapi, dash

# Full list in requirements.txt
```

#### **ğŸ› ï¸ Common Issues & Solutions**
```bash
# TA-Lib Installation Issues:
# macOS:
brew install ta-lib
pip install TA-Lib

# Ubuntu:
sudo apt-get install libta-lib-dev
pip install TA-Lib

# Windows:
# Download wheel from: https://www.lfd.uci.edu/~gohlke/pythonlibs/#ta-lib

# Memory Issues:
export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512

# Import Hanging:
# First PyTorch import may be slow (normal)
# Subsequent runs will be faster
```

### ğŸ¯ **Usage Examples**

#### **ğŸš€ Basic Usage**
```python
# Run with default settings (AAPL)
python src/ensemble_model.py

# The system will:
# 1. Download AAPL data automatically
# 2. Train Transformer + LSTM ensemble  
# 3. Generate predictions with uncertainty
# 4. Create interactive HTML visualizations
# 5. Display performance metrics
```

#### **ğŸ“Š Output Files**
```python
After running, check these folders:

visualizations/
â”œâ”€â”€ AAPL_dashboard.html    # Complete interactive dashboard
â”œâ”€â”€ AAPL_simple.html       # Clean prediction chart  
â””â”€â”€ AAPL_results.md        # Performance summary

models/
â”œâ”€â”€ AAPL_transformer.pth   # Trained Transformer weights
â””â”€â”€ AAPL_lstm.pth          # Trained LSTM weights

data/
â””â”€â”€ AAPL_enhanced.csv      # Processed data with all features
```

## ğŸ‰ **Ready to Predict the Future!**

This system combines cutting-edge AI research with practical financial engineering to create a robust, interpretable, and high-performance stock prediction tool.

**Key Strengths:**
- âœ… **State-of-the-art AI**: Transformer + LSTM ensemble
- âœ… **Comprehensive Features**: 21 engineered features from basic OHLCV
- âœ… **Uncertainty Quantification**: Know when to trust predictions
- âœ… **Interactive Visualizations**: Professional charts and analysis
- âœ… **Pure PyTorch**: Modern, unified framework
- âœ… **Real Data Only**: No fake or simulated features
- âœ… **Production Ready**: Model persistence, error handling, logging

**Perfect for:** Research, backtesting, educational purposes, and algorithm development.

ğŸš€ **Start predicting now:** `python src/ensemble_model.py` ğŸ“ˆğŸ¤–
